<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>6.大数据专栏 | HDFS详解 (理论知识)</title>
      <link href="/hadoop/hdfs/fdb7207.html"/>
      <url>/hadoop/hdfs/fdb7207.html</url>
      
        <content type="html"><![CDATA[<center><h3>大数据专栏 | HDFS详解（理论知识）</h3></center><hr><h3 id="HDFS-工作机制"><a href="#HDFS-工作机制" class="headerlink" title="HDFS 工作机制"></a>HDFS 工作机制</h3><blockquote><p>工作机制的学习主要是为加深对分布式系统的理解，以及增强遇到各种问题时的分析解决能力，形成一定的集群运维能力）</p><p>很多不是真正理解hadoop技术体系的人会常常觉得HDFS可用于网盘类应用，但实际并非如此。要想将技术准确用在恰当的地方，必须对技术有深刻的理解</p></blockquote><h4 id="概述："><a href="#概述：" class="headerlink" title="概述："></a>概述：</h4><ol><li><p>HDFS集群分为两大角色：<strong>NameNode、DataNode</strong></p></li><li><p><strong>NameNode</strong>负责管理整个文件系统的元数据</p></li><li><p><strong>DataNode</strong> 负责管理用户的文件数据块</p></li><li><p>文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台<strong>datanode</strong>上</p></li><li><p>每一个文件块可以有多个副本，并存放在不同的<strong>datanode</strong>上</p></li><li><p><strong>Datanode</strong>会定期向<strong>Namenode</strong>汇报自身所保存的文件block信息，而<strong>namenode</strong>则会负责保持文件的副本数量</p></li><li><p>HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向<strong>namenode</strong>申请来进行</p></li></ol><h3 id="HDFS-写数据流程"><a href="#HDFS-写数据流程" class="headerlink" title="HDFS 写数据流程"></a>HDFS 写数据流程</h3><blockquote><p>客户端要向HDFS写数据，首先要跟namenode通信以确认可以写文件并获得接收文件block的datanode，然后，客户端按顺序将文件逐个block传递给相应datanode，并由接收到block的datanode负责向其他datanode复制block的副本</p></blockquote><h4 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h4><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190801105059.png" alt></p><h4 id="流程说明"><a href="#流程说明" class="headerlink" title="流程说明"></a>流程说明</h4><p>1、根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在</p><p>2、namenode返回是否可以上传</p><p>3、client请求第一个 block该传输到哪些datanode服务器上</p><p>4、namenode返回3个datanode服务器ABC</p><p>5、client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端</p><p>6、client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答</p><p>7、当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。</p><h3 id="HDFS-读数据流程"><a href="#HDFS-读数据流程" class="headerlink" title="HDFS 读数据流程"></a>HDFS 读数据流程</h3><blockquote><p>客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件</p></blockquote><h4 id="流程图-1"><a href="#流程图-1" class="headerlink" title="流程图"></a>流程图</h4><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190801105233.png" alt></p><h4 id="流程说明-1"><a href="#流程说明-1" class="headerlink" title="流程说明"></a>流程说明</h4><p>1、跟namenode通信查询元数据，找到文件块所在的datanode服务器</p><p>2、挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流</p><p>3、datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）</p><p>4、客户端以packet为单位接收，现在本地缓存，然后写入目标文件</p><h3 id="NameNode-工作机制"><a href="#NameNode-工作机制" class="headerlink" title="NameNode 工作机制"></a>NameNode 工作机制</h3><blockquote><p><em>问题场景：</em></p><p><em>1、集群启动后，可以查看文件，但是上传文件时报错，打开web页面可看到namenode正处于safemode状态，怎么处理？</em></p><p><em>2、<strong>Namenode服务器的磁盘故障导致namenode宕机</strong>，<strong>如何挽救集群及数据</strong>？</em></p><p><em>3、<strong>Namenode是否可以有多个</strong>？namenode内存要配置多大？namenode跟集群数据存储能力有关系吗？</em></p><p><em>4、文件的blocksize究竟调大好还是调小好？</em></p><p><em>……</em></p><p><em>诸如此类问题的回答，都需要基于对namenode自身的工作原理的深刻理解</em></p></blockquote><h4 id="NameNode-职责"><a href="#NameNode-职责" class="headerlink" title="NameNode 职责"></a>NameNode 职责</h4><ul><li><p>负责客户端请求的响应</p></li><li><p>元数据的管理（查询，修改）</p></li></ul><h4 id="元数据管理"><a href="#元数据管理" class="headerlink" title="元数据管理"></a>元数据管理</h4><ul><li>namenode对数据的管理采用了三种存储形式：</li><li>内存元数据(NameSystem)</li><li>磁盘元数据镜像文件</li><li>数据操作日志文件（可通过日志运算出元数据）</li></ul><h4 id="元数据存储机制"><a href="#元数据存储机制" class="headerlink" title="元数据存储机制"></a>元数据存储机制</h4><p>内存中有一份完整的元数据(<strong>内存meta data</strong>)</p><p>磁盘有一个“准完整”的元数据镜像（<strong>fsimage</strong>）文件(在namenode的工作目录中)</p><p>用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（<strong>edits文件</strong>）</p><blockquote><p><em>注：当客户端对hdfs中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data中</em></p></blockquote><h4 id="元数据手动·查看"><a href="#元数据手动·查看" class="headerlink" title="元数据手动·查看"></a>元数据手动·查看</h4><p>可以通过hdfs的一个工具来查看edits中的信息</p><p>bin/hdfs oev -i edits -o edits.xml</p><p>bin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml</p><h4 id="元数据的checkpoint"><a href="#元数据的checkpoint" class="headerlink" title="元数据的checkpoint"></a>元数据的checkpoint</h4><p>每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）</p><h4 id="checkpoint-的详细过程"><a href="#checkpoint-的详细过程" class="headerlink" title="checkpoint 的详细过程"></a>checkpoint 的详细过程</h4><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190801110001.png" alt></p><h4 id="checkpoint-操作的触发条件配置参数"><a href="#checkpoint-操作的触发条件配置参数" class="headerlink" title="checkpoint 操作的触发条件配置参数"></a>checkpoint 操作的触发条件配置参数</h4><p>dfs.namenode.checkpoint.check.period=60  #检查触发条件是否满足的频率，60秒</p><p>dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary</p><p>#以上两个参数做checkpoint操作时，secondary namenode的本地工作目录</p><p>dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir} dfs.namenode.checkpoint.max-retries=3  #最大重试次数</p><p>dfs.namenode.checkpoint.period=3600  #两次checkpoint之间的时间间隔3600秒</p><p>dfs.namenode.checkpoint.txns=1000000 #两次checkpoint之间最大的操作记录</p><h3 id="DataNode-工作机制"><a href="#DataNode-工作机制" class="headerlink" title="DataNode 工作机制"></a>DataNode 工作机制</h3><blockquote><p><em>问题场景：</em></p><p><em>1、集群容量不够，怎么扩容？</em></p><p><em>2、如果有一些datanode宕机，该怎么办？</em></p><p><em>3、datanode明明已启动，但是集群中的可用datanode列表中就是没有，怎么办？</em></p><p><em>以上这类问题的解答，有赖于对datanode工作机制的深刻理解</em></p></blockquote><h3 id="Datanode工作职责："><a href="#Datanode工作职责：" class="headerlink" title="Datanode工作职责："></a>Datanode工作职责：</h3><p><strong>存储管理用户的文件块数据</strong></p><p>定期向namenode汇报自身所持有的block信息（通过心跳信息上报）</p><p>（这点很重要，因为，当集群中发生某些block副本失效时，集群如何恢复block初始副本数量的问题）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3600000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Determines block reporting interval in milliseconds.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>Datanode掉线判断时限参数</strong></p><p>datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：</p><p>​    timeout  = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval。</p><p>而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。</p><p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>heartbeat.recheck.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="观察验证DATANODE功能"><a href="#观察验证DATANODE功能" class="headerlink" title="观察验证DATANODE功能"></a>观察验证DATANODE功能</h2><p>上<strong>传一个文件，观察文件的block具体的物理存放情况：</strong></p><hr><p><strong>在每一台datanode机器上的这个目录中能找到文件的切块：</strong></p><p>/home/hadoop/app/hadoop-2.6.4/tmp/dfs/data/current/BP-193442119-192.168.2.120-1432457733977/current/finalized</p>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
          <category> hdfs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.大数据专栏 | HDFS详解 （命令行操作）</title>
      <link href="/hadoop/hdfs/a0a5b9a8.html"/>
      <url>/hadoop/hdfs/a0a5b9a8.html</url>
      
        <content type="html"><![CDATA[<center><h3>大数据专栏 | HDFS详解（命令行操作）</h3></center><hr><h3 id="HDFS基本概念"><a href="#HDFS基本概念" class="headerlink" title="HDFS基本概念"></a>HDFS基本概念</h3><ul><li><p><strong>前言：</strong></p><blockquote><p>l 设计思想</p><p>分而治之：将大文件、大批量文件，分布式存放在大量服务器上，<strong>以便于采取分而治之的方式对海量数据进行运算分析；</strong></p><p>l 在大数据系统中作用：</p><p>为各类分布式运算框架（如：mapreduce，spark，tez，……）提供数据存储服务</p><p>l 重点概念：文件切块，副本存放，元数据</p></blockquote></li><li><p>HDFS的概念和特性</p><p><strong>首先，它是一个文件系统</strong>，用于存储文件，通过统一的命名空间——目录树来定位文件</p><p><strong>其次，它是分布式的</strong>，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色；</p></li></ul><p>  <strong>重要特性如下：</strong></p><p>  （1）HDFS中的文件在物理上是<strong>分块存储（block）</strong>，块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M</p><p>  （2）HDFS文件系统会给客户端提供一个<strong>统一的抽象目录树</strong>，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data</p><p>  <strong>（3）目录结构及文件分块信息(元数据)</strong>的管理由namenode节点承担</p><p>  ——namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的datanode服务器）</p><p>  （4）文件的各个block的存储管理由datanode节点承担</p><p>  —- datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication）</p><p>  （5）HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改</p><p>  <strong>需要注意的是：</strong></p><blockquote><p>HDFS 适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高</p></blockquote><hr><h3 id="HDFS-基本操作"><a href="#HDFS-基本操作" class="headerlink" title="HDFS 基本操作"></a>HDFS 基本操作</h3><p>如果你还没有搭建Hadoop环境可参考这篇教程：<a href="https://blog.nowcoder.net/n/086c916e6f38417da4f8d8493fb125bc" target="_blank" rel="noopener">Hadoop3.1集群环境搭建</a></p><h4 id="HDFS命令行操作"><a href="#HDFS命令行操作" class="headerlink" title="HDFS命令行操作"></a>HDFS命令行操作</h4><p>首先执行 <code>start-all.sh</code> 启动Hadoop集群:</p><p>查看我的集群状态：执行：<code>hdfs dfsadmin -report</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">Configured Capacity: 36870701056 (34.34 GB)</span><br><span class="line">Present Capacity: 25769635840 (24.00 GB)</span><br><span class="line">DFS Remaining: 25769537536 (24.00 GB)</span><br><span class="line">DFS Used: 98304 (96 KB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">Under replicated blocks: 0</span><br><span class="line">Blocks with corrupt replicas: 0</span><br><span class="line">Missing blocks: 0</span><br><span class="line"></span><br><span class="line">-------------------------------------------------</span><br><span class="line">Live datanodes (2):</span><br><span class="line"></span><br><span class="line">Name: 192.168.25.202:50010 (mini62)</span><br><span class="line">Hostname: mini62</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 18435350528 (17.17 GB)</span><br><span class="line">DFS Used: 49152 (48 KB)</span><br><span class="line">Non DFS Used: 5182672896 (4.83 GB)</span><br><span class="line">DFS Remaining: 13252628480 (12.34 GB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">DFS Remaining%: 71.89%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Wed Jul 31 17:29:47 PDT 2019</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Name: 192.168.25.203:50010 (mini63)</span><br><span class="line">Hostname: mini63</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 18435350528 (17.17 GB)</span><br><span class="line">DFS Used: 49152 (48 KB)</span><br><span class="line">Non DFS Used: 5918392320 (5.51 GB)</span><br><span class="line">DFS Remaining: 12516909056 (11.66 GB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">DFS Remaining%: 67.90%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Wed Jul 31 17:29:47 PDT 2019</span><br></pre></td></tr></table></figure><p>可以看到我目前有两个 DataNode 节点：</p><p>HDFS 命令和 Linux 命令类似：</p><h4 id="HDFS-命令行操作演示："><a href="#HDFS-命令行操作演示：" class="headerlink" title="HDFS 命令行操作演示："></a>HDFS 命令行操作演示：</h4><p>hadoop fs  和 hdfs dfs 后接命令参数是一样的：关于这两个命令的区别可参看：<a href="https://stackoverflow.com/questions/18142960/whats-the-difference-between-hadoop-fs-shell-commands-and-hdfs-dfs-shell-co" target="_blank" rel="noopener">stackoverflow论坛</a></p><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190801085626.png" alt></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini61 ~]$ hadoop fs -ls /          # 查看 根目录 的文件</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2019-07-30 22:14 /wordcount</span><br><span class="line">[hadoop@mini61 ~]$ hadoop fs -lsr /         # 遍历查看 根目录 文件</span><br><span class="line">lsr: DEPRECATED: Please use 'ls -R' instead.</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2019-07-30 22:14 /wordcount</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2019-07-30 22:19 /wordcount/input</span><br><span class="line">-rw-r--r--   2 hadoop supergroup        705 2019-07-30 22:19 /wordcount/input/somewords.txt</span><br><span class="line">[hadoop@mini61 ~]$ hadoop fs -cat /wordcount/input/somewords.txt   # 查看文件内容</span><br><span class="line">pache? Hadoop? project develops open-source software for reliable, scalable, distributed computing.</span><br></pre></td></tr></table></figure><p>更多命令可以执行  <code>hadoop fs -help</code>  查看：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[hadoop@mini61 ~]$ hadoop fs -help</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">        [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">        [-checksum &lt;src&gt; ...]</span><br><span class="line">        [-chgrp [-R] GROUP PATH...]</span><br><span class="line">        [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">        [-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">        [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">        [-count [-q] [-h] &lt;path&gt; ...]</span><br><span class="line">        [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">        [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">        [-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">        [-du [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">        [-expunge]</span><br><span class="line">        [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">        [-getfacl [-R] &lt;path&gt;]</span><br><span class="line">        [-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">        [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">        [-help [cmd ...]]</span><br><span class="line">        [-ls [-d] [-h] [-R] [&lt;path&gt; ...]]</span><br><span class="line">        [-mkdir [-p] &lt;path&gt; ...]</span><br><span class="line">        [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">        [-mv &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">        [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">        [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">        [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">        [-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">        [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">        [-stat [format] &lt;path&gt; ...]</span><br><span class="line">        [-tail [-f] &lt;file&gt;]</span><br><span class="line">        [-test -[defsz] &lt;path&gt;]</span><br><span class="line">        [-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">        [-touchz &lt;path&gt; ...]</span><br><span class="line">        [-usage [cmd ...]]</span><br></pre></td></tr></table></figure><h4 id="HDFS-常用命令-介绍："><a href="#HDFS-常用命令-介绍：" class="headerlink" title="HDFS 常用命令 介绍："></a>HDFS 常用命令 介绍：</h4><table><thead><tr><th>-help             功能：输出这个命令参数手册</th></tr></thead><tbody><tr><td><strong>-ls</strong>                  功能：显示目录信息示例： hadoop fs -ls hdfs://hadoop-server01:9000/备注：这些参数中，所有的hdfs路径都可以简写–&gt;hadoop fs -ls /   等同于上一条命令的效果</td></tr><tr><td><strong>-mkdir</strong>              功能：在hdfs上创建目录示例：hadoop fs  -mkdir  -p  /aaa/bbb/cc/dd</td></tr><tr><td><strong>-moveFromLocal</strong>            功能：从本地剪切粘贴到hdfs示例：hadoop  fs  - moveFromLocal  /home/hadoop/a.txt  /aaa/bbb/cc/dd-moveToLocal              功能：从hdfs剪切粘贴到本地示例：hadoop  fs  - moveToLocal   /aaa/bbb/cc/dd  /home/hadoop/a.txt</td></tr><tr><td><strong>–appendToFile</strong>  功能：追加一个文件到已经存在的文件末尾示例：hadoop  fs  -appendToFile  ./hello.txt  hdfs://hadoop-server01:9000/hello.txt可以简写为：Hadoop  fs  -appendToFile  ./hello.txt  /hello.txt</td></tr><tr><td><strong>-cat</strong>  功能：显示文件内容  示例：hadoop fs -cat  /hello.txt -tail                 功能：显示一个文件的末尾示例：hadoop  fs  -tail  /weblog/access_log.1-text                  功能：以字符形式打印一个文件的内容示例：hadoop  fs  -text  /weblog/access_log.1</td></tr><tr><td><strong>-chgrp -chmod-chown</strong>功能：linux文件系统中的用法一样，对文件所属权限示例：hadoop  fs  -chmod  666  /hello.txthadoop  fs  -chown  someuser:somegrp   /hello.txt</td></tr><tr><td><strong>-copyFromLocal</strong>    功能：从本地文件系统中拷贝文件到hdfs路径去示例：hadoop  fs  -copyFromLocal  ./jdk.tar.gz  /aaa/-copyToLocal      功能：从hdfs拷贝到本地示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz</td></tr><tr><td><strong>-cp</strong>              功能：从hdfs的一个路径拷贝hdfs的另一个路径示例： hadoop  fs  -cp  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2 -mv                     功能：在hdfs目录中移动文件示例： hadoop  fs  -mv  /aaa/jdk.tar.gz  /</td></tr><tr><td><strong>-get</strong>              功能：等同于copyToLocal，就是从hdfs下载文件到本地示例：hadoop fs -get  /aaa/jdk.tar.gz-getmerge             功能：合并下载多个文件示例：比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,…hadoop fs -getmerge /aaa/log. ./log.sum</td></tr><tr><td><strong>-put</strong>                功能：等同于copyFromLocal示例：hadoop  fs  -put  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2</td></tr><tr><td><strong>-rm</strong>                功能：删除文件或文件夹示例：hadoop fs -rm -r /aaa/bbb/ -rmdir                 功能：删除空目录示例：hadoop  fs  -rmdir   /aaa/bbb/ccc</td></tr><tr><td><strong>-df</strong>               功能：统计文件系统的可用空间信息示例：hadoop  fs  -df  -h  / -du 功能：统计文件夹的大小信息示例：hadoop  fs  -du  -s  -h /aaa/</td></tr><tr><td><strong>-count</strong>         功能：统计一个指定目录下的文件节点数量示例：hadoop fs -count /aaa/</td></tr><tr><td><strong>-setrep</strong>                功能：设置hdfs中文件的副本数量示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz&lt;这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量&gt;</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
          <category> hdfs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.MapReduce使用</title>
      <link href="/hadoop/mapreduce/5ce6e6bd.html"/>
      <url>/hadoop/mapreduce/5ce6e6bd.html</url>
      
        <content type="html"><![CDATA[<center><h3>大数据专栏 | MapReduce的使用</h3></center><hr><h2 id="MapReduce的使用"><a href="#MapReduce的使用" class="headerlink" title="MapReduce的使用"></a>MapReduce的使用</h2><blockquote><p>mapreduce是hadoop中的分布式运算编程框架，只要按照其编程规范，只需要编写少量的业务逻辑代码即可实现一个强大的海量数据并发处理程序</p></blockquote><h4 id="1、需求"><a href="#1、需求" class="headerlink" title="1、需求"></a><strong>1、需求</strong></h4><p>从大量（比如T级别）文本文件中，统计出每一个单词出现的总次数</p><h4 id="2、mapreduce实现思路"><a href="#2、mapreduce实现思路" class="headerlink" title="2、mapreduce实现思路"></a><strong>2、mapreduce实现思路</strong></h4><p><strong>Map阶段：</strong></p><ol><li>a) 从HDFS的源数据文件中逐行读取数据</li><li>b) 将每一行数据切分出单词</li><li>c) 为每一个单词构造一个键值对(单词，1)</li><li>d) 将键值对发送给reduce</li></ol><p><strong>Reduce阶段：</strong></p><ol><li>a) 接收map阶段输出的单词键值对</li><li>b) 将相同单词的键值对汇聚成一组</li><li>c) 对每一组，遍历组中的所有“值”，累加求和，即得到每一个单词的总次数</li><li>d) 将(单词，总次数)输出到HDFS的文件中</li></ol><h4 id="3、-具体编码实现"><a href="#3、-具体编码实现" class="headerlink" title="3、 具体编码实现"></a><strong>3、 具体编码实现</strong></h4><ol><li>定义一个mapper类</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//首先要定义四个泛型的类型</span></span><br><span class="line"><span class="comment">//keyin:  LongWritable    valuein: Text</span></span><br><span class="line"><span class="comment">//keyout: Text            valueout:IntWritable</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"><span class="comment">//map方法的生命周期：  框架每传一行数据就被调用一次</span></span><br><span class="line"><span class="comment">//key :  这一行的起始点在文件中的偏移量</span></span><br><span class="line"><span class="comment">//value: 这一行的内容</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//拿到一行数据转换为string</span></span><br><span class="line">String line = value.toString();</span><br><span class="line"><span class="comment">//将这一行切分出各个单词</span></span><br><span class="line">String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line"><span class="comment">//遍历数组，输出&lt;单词，1&gt;</span></span><br><span class="line"><span class="keyword">for</span>(String word:words)&#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>定义一个reducer类</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//生命周期：框架每传递进来一个kv 组，reduce方法被调用一次</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//定义一个计数器</span></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="comment">//遍历这一组kv的所有v，累加到count中</span></span><br><span class="line"><span class="keyword">for</span>(IntWritable value:values)&#123;</span><br><span class="line">count += value.get();</span><br><span class="line">&#125;</span><br><span class="line">context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li>定义一个主类，用来描述job并提交job</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountRunner</span> </span>&#123;</span><br><span class="line"><span class="comment">//把业务逻辑相关的信息（哪个是mapper，哪个是reducer，要处理的数据在哪里，输出的结果放哪里。。。。。。）描述成一个job对象</span></span><br><span class="line"><span class="comment">//把这个描述好的job提交给集群去运行</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">Job wcjob = Job.getInstance(conf);</span><br><span class="line"><span class="comment">//指定我这个job所在的jar包</span></span><br><span class="line"><span class="comment">//wcjob.setJar("/home/hadoop/wordcount.jar");</span></span><br><span class="line">wcjob.setJarByClass(WordCountRunner.class);</span><br><span class="line"></span><br><span class="line">wcjob.setMapperClass(WordCountMapper.class);</span><br><span class="line">wcjob.setReducerClass(WordCountReducer.class);</span><br><span class="line"><span class="comment">//设置我们的业务逻辑Mapper类的输出key和value的数据类型</span></span><br><span class="line">wcjob.setMapOutputKeyClass(Text.class);</span><br><span class="line">wcjob.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"><span class="comment">//设置我们的业务逻辑Reducer类的输出key和value的数据类型</span></span><br><span class="line">wcjob.setOutputKeyClass(Text.class);</span><br><span class="line">wcjob.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//指定要处理的数据所在的位置</span></span><br><span class="line">FileInputFormat.setInputPaths(wcjob, <span class="string">"hdfs://hdp-server01:9000/wordcount/data/big.txt"</span>);</span><br><span class="line"><span class="comment">//指定处理完成之后的结果所保存的位置</span></span><br><span class="line">FileOutputFormat.setOutputPath(wcjob, <span class="keyword">new</span> Path(<span class="string">"hdfs://hdp-server01:9000/wordcount/output/"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//向yarn集群提交这个job</span></span><br><span class="line"><span class="keyword">boolean</span> res = wcjob.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(res?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="程序打包运行"><a href="#程序打包运行" class="headerlink" title="程序打包运行"></a><strong>程序打包运行</strong></h3><p>将程序打包</p><p>准备输入数据  输入一下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> vi  /home/hadoop/test.txt</span><br></pre></td></tr></table></figure><blockquote><p>Hello tom</p><p>Hello jim</p><p>Hello ketty</p><p>Hello world</p><p>Ketty tom</p></blockquote><ol start="4"><li>在hdfs上创建输入数据文件夹：</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hadoop   fs  mkdir  -p  /wordcount/input</span><br></pre></td></tr></table></figure><p>将words.txt上传到hdfs上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hadoop  fs  –put  /home/hadoop/words.txt  /wordcount/input</span><br></pre></td></tr></table></figure><p>查看是否上传成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hadoop fs -ls /wordcount/input</span><br></pre></td></tr></table></figure><ol start="5"><li>将程序jar包上传到集群的任意一台服务器上</li><li>使用命令启动执行wordcount程序jar包</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hadoop jar wordcount.jar +java代码完整包名 /wordcount/input /wordcount/out</span><br></pre></td></tr></table></figure><ol start="7"><li>查看执行结果</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hadoop fs –cat /wordcount/out/part-r-00000</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
          <category> mapreduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.Hadoop环境测试</title>
      <link href="/hadoop/b2428c70.html"/>
      <url>/hadoop/b2428c70.html</url>
      
        <content type="html"><![CDATA[<center><h3>大数据专栏 | Hadoop环境测试</h3></center><hr><h3 id="文件上传测试"><a href="#文件上传测试" class="headerlink" title="文件上传测试"></a>文件上传测试</h3><h4 id="上传文件到HDFS"><a href="#上传文件到HDFS" class="headerlink" title="上传文件到HDFS"></a>上传文件到HDFS</h4><p><strong>从本地上传一个文本文件到hdfs的/wordcount/input目录下</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini61 ~]$ hadoop fs -mkdir -p /wordcount/input    # 创建目录</span><br><span class="line">[hadoop@mini61 ~]$ hadoop fs -put ./somewords.txt /wordcount/input  # 将本地文件 somewords.txt 上传到 HDFS</span><br></pre></td></tr></table></figure><blockquote><p>遇到的问题：上传文件报错： hdfs.DFSClient: Exception in createBlockOutputStream<br>java.net.NoRouteToHostException: 没有找到路由</p><p>put: File /wordcount/input/somewords.txt.<em>COPYING</em> could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and 2 node(s) are excluded in this operation.</p><p><strong>请检查防火墙是否关闭</strong></p></blockquote><p>验证是否上传成功：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini61 ~]$ hadoop fs -ls /wordcount/input</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup        705 2019-07-30 22:19 /wordcount/input/somewords.txt</span><br></pre></td></tr></table></figure><h3 id="集群使用初步"><a href="#集群使用初步" class="headerlink" title="集群使用初步"></a>集群使用初步</h3><h4 id="HDFS使用"><a href="#HDFS使用" class="headerlink" title="HDFS使用"></a>HDFS使用</h4><p><strong>查看集群状态</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命令：   hdfs  dfsadmin  –report</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731131432.png" alt></p><blockquote><p>可以看出，集群共有3个datanode可用</p><p>也可打开web控制台查看HDFS集群信息，在浏览器打开<strong>192.168.25.201:50070/</strong></p></blockquote><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731132901.png" alt></p><p><strong>上传文件到HDFS</strong></p><ul><li>查看HDFS中的目录信息</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命令：   hadoop  fs  –ls  +要查看的目录</span><br></pre></td></tr></table></figure><ul><li>上传文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命令：   hadoop  fs  -put  +要上传的文件  存放在HDFS上的目录</span><br></pre></td></tr></table></figure><ul><li>从HDFS下载文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命令：  hadoop  fs  -get  + filename</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.Hadoop环境搭建</title>
      <link href="/hadoop/27464cc.html"/>
      <url>/hadoop/27464cc.html</url>
      
        <content type="html"><![CDATA[<center><h3>大数据专栏 | Hadoop集群环境搭建</h3></center><hr><p><strong>下面为简化版搭建教程</strong></p><h4 id="附软件下载地址"><a href="#附软件下载地址" class="headerlink" title="附软件下载地址:"></a>附软件下载地址:</h4><p>Hadoop官网下载：<a href="https://hadoop.apache.org/releases.html" target="_blank" rel="noopener">https://hadoop.apache.org/releases.html</a></p><p>JDK下载：<a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p><p>CentOS下载：<a href="http://isoredirect.centos.org/centos/" target="_blank" rel="noopener">http://isoredirect.centos.org/centos/</a></p><p>Hadoop官网安装文档：<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html</a></p><p>一般练习时可以搭建伪分布式(单机版)来学习也是可以的。</p><h3 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h3><p>HADOOP集群具体来说包含两个集群：<strong>HDFS集群和YARN集群，两者逻辑上分离，但物理上常在一起</strong></p><p><strong>HDFS集群：</strong></p><p>负责海量数据的存储，集群中的角色主要有 <code>NameNode / DataNode</code></p><p><strong>YARN集群：</strong></p><p>负责海量数据运算时的资源调度，集群中的角色主要有 <code>ResourceManager /NodeManager</code></p><p>(那mapreduce是什么呢？它其实是一个应用程序开发包)</p><ul><li>本集群搭建案例，以3节点为例进行搭建，角色分配如下：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mini61    NameNode  SecondaryNameNode   ResourceManager </span><br><span class="line">mini62   DataNode    NodeManager</span><br><span class="line">mini63DataNode    NodeManager</span><br></pre></td></tr></table></figure><h4 id="服务器准备"><a href="#服务器准备" class="headerlink" title="服务器准备"></a>服务器准备</h4><ul><li><p>Vmware 12</p></li><li><p>Centos  6.7  64bit</p></li></ul><h4 id="网络环境准备"><a href="#网络环境准备" class="headerlink" title="网络环境准备"></a><strong>网络环境准备</strong></h4><ul><li><p>采用NAT方式联网</p></li><li><p>网关地址：192.168.25.1</p></li><li><p>3个服务器节点IP地址：192.168.25.201、192.168.25.202、192.168.25.203</p></li><li><p>子网掩码：255.255.255.0</p></li></ul><h4 id="服务器系统设置"><a href="#服务器系统设置" class="headerlink" title="服务器系统设置"></a>服务器系统设置</h4><ul><li><p>添加hadoop用户 【也可以直接使用root用户】 （小白可参考百度：<a href="https://jingyan.baidu.com/article/19192ad81e2919e53f57074d.html" target="_blank" rel="noopener">centos系统添加/删除用户和用户组的例子</a>）</p></li><li><p>为hadoop用户分配sudoer权限</p><p>切换为 <code>root</code> 用户修改配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">su root</span><br><span class="line">vi /etc/sudoers</span><br></pre></td></tr></table></figure><p>添加用户，如图：</p></li></ul><p>  <img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731112010.png" alt></p><ul><li><p>同步时间(时间正常就不需要)</p></li><li><p><strong>分别</strong>设置主机名</p><p>mini61</p><p>mini62</p><p>mini63</p><p>修改配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@mini61 hadoop]# vi /etc/sysconfig/network</span><br></pre></td></tr></table></figure></li></ul><p>  <img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731112215.png" alt></p><p><strong>配置内网域名映射：</strong></p><blockquote><p>192.168.25.201          mini61</p><p> 192.168.25.202          mini62</p><p>192.168.25.203          mini63</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@mini61 hadoop]# vi /etc/hosts</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731112408.png" alt></p><h4 id="配置ssh免密登陆"><a href="#配置ssh免密登陆" class="headerlink" title="配置ssh免密登陆"></a><strong>配置ssh免密登陆</strong></h4><p>下面为简单版，如首次接触Linux,请自行搜索查找教程</p><p>配置ssh免登陆</p><pre><code>#生成ssh免登陆密钥#进入到我的home目录cd ~/.ssh</code></pre><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa （四个回车）</span><br><span class="line">执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</span><br><span class="line">将公钥拷贝到要免密登陆的目标机器上</span><br><span class="line">ssh-copy-id localhost</span><br><span class="line">---------------------------</span><br><span class="line">ssh免登陆：</span><br><span class="line">生成key:</span><br><span class="line">ssh-keygen</span><br><span class="line">复制从A复制到B上:</span><br><span class="line">ssh-copy-id B</span><br><span class="line">验证：</span><br><span class="line">ssh localhost/exit，ps -e|grep ssh</span><br><span class="line">ssh A  #在B中执行</span><br></pre></td></tr></table></figure><h4 id="配置-关闭防火墙"><a href="#配置-关闭防火墙" class="headerlink" title="配置/关闭防火墙"></a><strong>配置/关闭防火墙</strong></h4><p>下面为 <code>CentOS 6</code> 版本关闭防火墙的方法，如使用的是 <code>CentOS 7</code> 请自行百度</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>查看防火墙状态</span><br><span class="line">service iptables status</span><br><span class="line"><span class="meta">#</span>关闭防火墙</span><br><span class="line">service iptables stop</span><br><span class="line"><span class="meta">#</span>查看防火墙开机启动状态</span><br><span class="line">chkconfig iptables --list</span><br><span class="line"><span class="meta">#</span>关闭防火墙开机启动</span><br><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><h4 id="Jdk环境安装"><a href="#Jdk环境安装" class="headerlink" title="Jdk环境安装"></a>Jdk环境安装</h4><p><strong>如未安装过</strong>(说明还没有学习过<code>Java</code>,那么建议先学习 <code>Java</code> 基础)，请参考百度：<a href="https://jingyan.baidu.com/article/2a138328fda0b2074a134f20.html" target="_blank" rel="noopener">JDK安装</a></p><ul><li><p>上传jdk安装包</p></li><li><p>规划安装目录  <code>/home/hadoop/app/jdk_1.7.45</code></p></li><li><p>解压安装包</p></li><li><p>配置环境变量 /etc/profile</p></li></ul><h4 id="HADOOP安装部署"><a href="#HADOOP安装部署" class="headerlink" title="HADOOP安装部署"></a>HADOOP安装部署</h4><p>配置好一台后使用 <code>scp</code> 命令发送到另外两台即可</p><blockquote><p>下载链接前面已提及</p></blockquote><ul><li>上传HADOOP安装包</li><li>规划安装目录  <code>/home/hadoop/app/hadoop-2.6.4</code></li><li>解压安装包</li></ul><p>安装方法类似 <code>JDK</code> 本次我下载的是hadoop <code>2.6.4</code> 版本</p><p>本次配置 <code>/etc/profile</code> 文件参考如下(请根据需要修改)：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/hadoop/app/jdk1.7.0_45</span><br><span class="line">export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.4</span><br><span class="line"></span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</span><br></pre></td></tr></table></figure><h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><p>进入 <strong>$HADOOP_HOME/etc/hadoop/</strong> 目录下：</p><p><code>vi  hadoop-env.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> The java implementation to use.</span><br><span class="line">export JAVA_HOME=/home/hadoop/apps/jdk1.7.0_45</span><br></pre></td></tr></table></figure><p><code>vi  core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mini61:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadpdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vi  hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadpdata/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadpdata/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.secondary.http.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mini61:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>mapred-site.xml 需要先修改名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure><p><code>vi  mapred-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vi  yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mini61<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vi  salves</code>  （作为DataNode的主机名）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mini62</span><br><span class="line">mini63</span><br></pre></td></tr></table></figure><h4 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h4><p>切回 <strong>hadoop</strong> 用户</p><p><code>初始化HDFS</code></p><p>在 <code>mini61</code> 上执行格式化 这里 <code>mini61</code> 我作为 <code>master</code>， 另外两台作为从节点 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop  namenode  -format</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731120058.png" alt></p><p><strong>如图所示就启动<code>格式化</code>成功了。</strong></p><p><strong><code>启动HDFS</code></strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure><p><code>启动YARN</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><p>如果直接执行 <code>start-all.sh</code> 就代表 同时启动 <code>HDFS</code> 和 <code>Yarn</code></p><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731120245.png" alt></p><p>三个节点分别执行 jps  查看进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini61 ~]$ jps</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731123830.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini62 ~]$ jps</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731121516.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini63 ~]$ jps</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731123932.png" alt></p><p>详细教程可参看：<a href="https://blog.csdn.net/iflytop/article/details/82413694" target="_blank" rel="noopener">Hadoop集群搭建</a>   （适合小白）</p><p><strong>注意：本人也是初学者，作为学习笔记，如有错漏，敬请见谅。</strong></p>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.认识Hadoop</title>
      <link href="/hadoop/f868e049.html"/>
      <url>/hadoop/f868e049.html</url>
      
        <content type="html"><![CDATA[<center><h3>大数据专栏 | 认识Hadoop</h3></center><hr><h3 id="Hadoop简介"><a href="#Hadoop简介" class="headerlink" title="Hadoop简介"></a>Hadoop简介</h3><ul><li><p>HADOOP是apache旗下的一套开源<strong>软件平台</strong></p></li><li><p>HADOOP提供的功能：利用服务器集群，根据用户的自定义业务逻辑，<strong>对海量数据进行分布式处理</strong></p></li><li><p>HADOOP的核心组件有</p><ul><li><p>HDFS（分布式文件系统）</p></li><li><p>YARN（运算资源调度系统）</p></li><li><p>MAPREDUCE（分布式运算编程框架）</p></li></ul></li><li><p>广义上来说，HADOOP通常是指一个更广泛的概念——HADOOP生态圈</p></li></ul><h3 id="Hadoop产生背景"><a href="#Hadoop产生背景" class="headerlink" title="Hadoop产生背景"></a>Hadoop产生背景</h3><ul><li><p>Hadoop<strong>最早起源于Nutch</strong>。Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能，但随着抓取网页数量的增加，<strong>遇到了严重的可扩展性问题——</strong>如何解决数十亿网页的存储和索引问题。</p></li><li><p>2003年、2004年<strong>谷歌发表的两篇论文为该问题提供了可行的解决方案</strong>。</p><ul><li><p>分布式文件系统（GFS），可用于处理海量网页的<strong>存储</strong></p></li><li><p>分布式计算框架MAPREDUCE，可用于处理海量网页的<strong>索引计算</strong>问题。</p></li></ul></li><li><p>Nutch的开发人员完成了相应的<strong>开源实现HDFS和MAPREDUCE</strong>，并从Nutch中剥离成为独立项目Hadoop，到2008年1月，Hadoop成为Apache顶级项目，迎来了它的快速发展期。</p></li></ul><h3 id="Hadoop在大数据、云计算中的位置和关系"><a href="#Hadoop在大数据、云计算中的位置和关系" class="headerlink" title="Hadoop在大数据、云计算中的位置和关系"></a>Hadoop在大数据、云计算中的位置和关系</h3><ul><li><p>云计算是分布式计算、并行计算、网格计算、多核计算、网络存储、虚拟化、负载均衡等传统计算机技术和互联网技术融合发展的产物。借助IaaS(基础设施即服务)、PaaS(平台即服务)、SaaS（软件即服务）等业务模式，把强大的计算能力提供给终端用户。</p></li><li><p>现阶段，云计算的<strong>两大底层支撑技术</strong>为“<strong>虚拟化</strong>”和“<strong>大数据技术</strong>”</p></li><li><p>而Hadoop则是云计算的PaaS层的解决方案之一，并不等同于PaaS，更不等同于云计算本身。</p></li></ul><h3 id="Hadoop应用案例"><a href="#Hadoop应用案例" class="headerlink" title="Hadoop应用案例"></a>Hadoop应用案例</h3><ul><li><strong>应用于数据服务基础平台建设</strong></li><li><strong>用于用户画像</strong></li><li><strong>用于网站点击流日志数据挖掘</strong></li></ul><h3 id="Hadoop生态圈重点组件"><a href="#Hadoop生态圈重点组件" class="headerlink" title="Hadoop生态圈重点组件"></a>Hadoop生态圈重点组件</h3><p>重点组件：</p><ul><li><p>HDFS：分布式文件系统</p></li><li><p>MAPREDUCE：分布式运算程序开发框架</p></li><li><p>HIVE：基于大数据技术（文件系统+运算框架）的SQL数据仓库工具</p></li><li><p>HBASE：基于HADOOP的分布式海量数据库</p></li><li><p><strong>ZOOKEEPER：分布式协调服务基础组件</strong></p></li><li><p>Mahout：基于mapreduce/spark/flink等分布式运算框架的机器学习算法库</p></li><li><p>Oozie：工作流调度框架</p></li><li><p>Sqoop：数据导入导出工具</p></li><li><p>Flume：日志数据采集框架</p></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
