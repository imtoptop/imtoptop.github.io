<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>4.MapReduce使用</title>
      <link href="/hadoop/mapreduce/5ce6e6bd.html"/>
      <url>/hadoop/mapreduce/5ce6e6bd.html</url>
      
        <content type="html"><![CDATA[<center><h3>大数据专栏 | MapReduce的使用</h3></center><hr><h2 id="MapReduce的使用"><a href="#MapReduce的使用" class="headerlink" title="MapReduce的使用"></a>MapReduce的使用</h2><blockquote><p>mapreduce是hadoop中的分布式运算编程框架，只要按照其编程规范，只需要编写少量的业务逻辑代码即可实现一个强大的海量数据并发处理程序</p></blockquote><h4 id="1、需求"><a href="#1、需求" class="headerlink" title="1、需求"></a><strong>1、需求</strong></h4><p>从大量（比如T级别）文本文件中，统计出每一个单词出现的总次数</p><h4 id="2、mapreduce实现思路"><a href="#2、mapreduce实现思路" class="headerlink" title="2、mapreduce实现思路"></a><strong>2、mapreduce实现思路</strong></h4><p><strong>Map阶段：</strong></p><ol><li>a) 从HDFS的源数据文件中逐行读取数据</li><li>b) 将每一行数据切分出单词</li><li>c) 为每一个单词构造一个键值对(单词，1)</li><li>d) 将键值对发送给reduce</li></ol><p><strong>Reduce阶段：</strong></p><ol><li>a) 接收map阶段输出的单词键值对</li><li>b) 将相同单词的键值对汇聚成一组</li><li>c) 对每一组，遍历组中的所有“值”，累加求和，即得到每一个单词的总次数</li><li>d) 将(单词，总次数)输出到HDFS的文件中</li></ol><h4 id="3、-具体编码实现"><a href="#3、-具体编码实现" class="headerlink" title="3、 具体编码实现"></a><strong>3、 具体编码实现</strong></h4><ol><li>定义一个mapper类</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//首先要定义四个泛型的类型</span></span><br><span class="line"><span class="comment">//keyin:  LongWritable    valuein: Text</span></span><br><span class="line"><span class="comment">//keyout: Text            valueout:IntWritable</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"><span class="comment">//map方法的生命周期：  框架每传一行数据就被调用一次</span></span><br><span class="line"><span class="comment">//key :  这一行的起始点在文件中的偏移量</span></span><br><span class="line"><span class="comment">//value: 这一行的内容</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//拿到一行数据转换为string</span></span><br><span class="line">String line = value.toString();</span><br><span class="line"><span class="comment">//将这一行切分出各个单词</span></span><br><span class="line">String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line"><span class="comment">//遍历数组，输出&lt;单词，1&gt;</span></span><br><span class="line"><span class="keyword">for</span>(String word:words)&#123;</span><br><span class="line">context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>定义一个reducer类</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//生命周期：框架每传递进来一个kv 组，reduce方法被调用一次</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="comment">//定义一个计数器</span></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="comment">//遍历这一组kv的所有v，累加到count中</span></span><br><span class="line"><span class="keyword">for</span>(IntWritable value:values)&#123;</span><br><span class="line">count += value.get();</span><br><span class="line">&#125;</span><br><span class="line">context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="3"><li>定义一个主类，用来描述job并提交job</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountRunner</span> </span>&#123;</span><br><span class="line"><span class="comment">//把业务逻辑相关的信息（哪个是mapper，哪个是reducer，要处理的数据在哪里，输出的结果放哪里。。。。。。）描述成一个job对象</span></span><br><span class="line"><span class="comment">//把这个描述好的job提交给集群去运行</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">Job wcjob = Job.getInstance(conf);</span><br><span class="line"><span class="comment">//指定我这个job所在的jar包</span></span><br><span class="line"><span class="comment">//wcjob.setJar("/home/hadoop/wordcount.jar");</span></span><br><span class="line">wcjob.setJarByClass(WordCountRunner.class);</span><br><span class="line"></span><br><span class="line">wcjob.setMapperClass(WordCountMapper.class);</span><br><span class="line">wcjob.setReducerClass(WordCountReducer.class);</span><br><span class="line"><span class="comment">//设置我们的业务逻辑Mapper类的输出key和value的数据类型</span></span><br><span class="line">wcjob.setMapOutputKeyClass(Text.class);</span><br><span class="line">wcjob.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"><span class="comment">//设置我们的业务逻辑Reducer类的输出key和value的数据类型</span></span><br><span class="line">wcjob.setOutputKeyClass(Text.class);</span><br><span class="line">wcjob.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//指定要处理的数据所在的位置</span></span><br><span class="line">FileInputFormat.setInputPaths(wcjob, <span class="string">"hdfs://hdp-server01:9000/wordcount/data/big.txt"</span>);</span><br><span class="line"><span class="comment">//指定处理完成之后的结果所保存的位置</span></span><br><span class="line">FileOutputFormat.setOutputPath(wcjob, <span class="keyword">new</span> Path(<span class="string">"hdfs://hdp-server01:9000/wordcount/output/"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//向yarn集群提交这个job</span></span><br><span class="line"><span class="keyword">boolean</span> res = wcjob.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">System.exit(res?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="程序打包运行"><a href="#程序打包运行" class="headerlink" title="程序打包运行"></a><strong>程序打包运行</strong></h3><p>将程序打包</p><p>准备输入数据  输入一下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> vi  /home/hadoop/test.txt</span><br></pre></td></tr></table></figure><blockquote><p>Hello tom</p><p>Hello jim</p><p>Hello ketty</p><p>Hello world</p><p>Ketty tom</p></blockquote><ol start="4"><li>在hdfs上创建输入数据文件夹：</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hadoop   fs  mkdir  -p  /wordcount/input</span><br></pre></td></tr></table></figure><p>将words.txt上传到hdfs上</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hadoop  fs  –put  /home/hadoop/words.txt  /wordcount/input</span><br></pre></td></tr></table></figure><p>查看是否上传成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hadoop fs -ls /wordcount/input</span><br></pre></td></tr></table></figure><ol start="5"><li>将程序jar包上传到集群的任意一台服务器上</li><li>使用命令启动执行wordcount程序jar包</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hadoop jar wordcount.jar +java代码完整包名 /wordcount/input /wordcount/out</span><br></pre></td></tr></table></figure><ol start="7"><li>查看执行结果</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> hadoop fs –cat /wordcount/out/part-r-00000</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
          <category> mapreduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.Hadoop环境测试</title>
      <link href="/hadoop/b2428c70.html"/>
      <url>/hadoop/b2428c70.html</url>
      
        <content type="html"><![CDATA[<center><h3>大数据专栏 | Hadoop环境测试</h3></center><hr><h3 id="文件上传测试"><a href="#文件上传测试" class="headerlink" title="文件上传测试"></a>文件上传测试</h3><h4 id="上传文件到HDFS"><a href="#上传文件到HDFS" class="headerlink" title="上传文件到HDFS"></a>上传文件到HDFS</h4><p><strong>从本地上传一个文本文件到hdfs的/wordcount/input目录下</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini61 ~]$ hadoop fs -mkdir -p /wordcount/input    # 创建目录</span><br><span class="line">[hadoop@mini61 ~]$ hadoop fs -put ./somewords.txt /wordcount/input  # 将本地文件 somewords.txt 上传到 HDFS</span><br></pre></td></tr></table></figure><blockquote><p>遇到的问题：上传文件报错： hdfs.DFSClient: Exception in createBlockOutputStream<br>java.net.NoRouteToHostException: 没有找到路由</p><p>put: File /wordcount/input/somewords.txt.<em>COPYING</em> could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and 2 node(s) are excluded in this operation.</p><p><strong>请检查防火墙是否关闭</strong></p></blockquote><p>验证是否上传成功：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini61 ~]$ hadoop fs -ls /wordcount/input</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 hadoop supergroup        705 2019-07-30 22:19 /wordcount/input/somewords.txt</span><br></pre></td></tr></table></figure><h3 id="集群使用初步"><a href="#集群使用初步" class="headerlink" title="集群使用初步"></a>集群使用初步</h3><h4 id="HDFS使用"><a href="#HDFS使用" class="headerlink" title="HDFS使用"></a>HDFS使用</h4><p><strong>查看集群状态</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命令：   hdfs  dfsadmin  –report</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731131432.png" alt></p><blockquote><p>可以看出，集群共有3个datanode可用</p><p>也可打开web控制台查看HDFS集群信息，在浏览器打开<strong>192.168.25.201:50070/</strong></p></blockquote><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731132901.png" alt></p><p><strong>上传文件到HDFS</strong></p><ul><li>查看HDFS中的目录信息</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命令：   hadoop  fs  –ls  +要查看的目录</span><br></pre></td></tr></table></figure><ul><li>上传文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命令：   hadoop  fs  -put  +要上传的文件  存放在HDFS上的目录</span><br></pre></td></tr></table></figure><ul><li>从HDFS下载文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命令：  hadoop  fs  -get  + filename</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.Hadoop环境搭建</title>
      <link href="/hadoop/27464cc.html"/>
      <url>/hadoop/27464cc.html</url>
      
        <content type="html"><![CDATA[<center><h3>大数据专栏 | Hadoop集群环境搭建</h3></center><hr><p><strong>下面为简化版搭建教程</strong></p><h4 id="附软件下载地址"><a href="#附软件下载地址" class="headerlink" title="附软件下载地址:"></a>附软件下载地址:</h4><p>Hadoop官网下载：<a href="https://hadoop.apache.org/releases.html" target="_blank" rel="noopener">https://hadoop.apache.org/releases.html</a></p><p>JDK下载：<a href="https://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p><p>CentOS下载：<a href="http://isoredirect.centos.org/centos/" target="_blank" rel="noopener">http://isoredirect.centos.org/centos/</a></p><p>Hadoop官网安装文档：<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html</a></p><p>一般练习时可以搭建伪分布式(单机版)来学习也是可以的。</p><h3 id="集群搭建"><a href="#集群搭建" class="headerlink" title="集群搭建"></a>集群搭建</h3><p>HADOOP集群具体来说包含两个集群：<strong>HDFS集群和YARN集群，两者逻辑上分离，但物理上常在一起</strong></p><p><strong>HDFS集群：</strong></p><p>负责海量数据的存储，集群中的角色主要有 <code>NameNode / DataNode</code></p><p><strong>YARN集群：</strong></p><p>负责海量数据运算时的资源调度，集群中的角色主要有 <code>ResourceManager /NodeManager</code></p><p>(那mapreduce是什么呢？它其实是一个应用程序开发包)</p><ul><li>本集群搭建案例，以3节点为例进行搭建，角色分配如下：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mini61    NameNode  SecondaryNameNode   ResourceManager </span><br><span class="line">mini62   DataNode    NodeManager</span><br><span class="line">mini63DataNode    NodeManager</span><br></pre></td></tr></table></figure><h4 id="服务器准备"><a href="#服务器准备" class="headerlink" title="服务器准备"></a>服务器准备</h4><ul><li><p>Vmware 12</p></li><li><p>Centos  6.7  64bit</p></li></ul><h4 id="网络环境准备"><a href="#网络环境准备" class="headerlink" title="网络环境准备"></a><strong>网络环境准备</strong></h4><ul><li><p>采用NAT方式联网</p></li><li><p>网关地址：192.168.25.1</p></li><li><p>3个服务器节点IP地址：192.168.25.201、192.168.25.202、192.168.25.203</p></li><li><p>子网掩码：255.255.255.0</p></li></ul><h4 id="服务器系统设置"><a href="#服务器系统设置" class="headerlink" title="服务器系统设置"></a>服务器系统设置</h4><ul><li><p>添加hadoop用户 【也可以直接使用root用户】 （小白可参考百度：<a href="https://jingyan.baidu.com/article/19192ad81e2919e53f57074d.html" target="_blank" rel="noopener">centos系统添加/删除用户和用户组的例子</a>）</p></li><li><p>为hadoop用户分配sudoer权限</p><p>切换为 <code>root</code> 用户修改配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">su root</span><br><span class="line">vi /etc/sudoers</span><br></pre></td></tr></table></figure><p>添加用户，如图：</p></li></ul><p>  <img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731112010.png" alt></p><ul><li><p>同步时间(时间正常就不需要)</p></li><li><p><strong>分别</strong>设置主机名</p><p>mini61</p><p>mini62</p><p>mini63</p><p>修改配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@mini61 hadoop]# vi /etc/sysconfig/network</span><br></pre></td></tr></table></figure></li></ul><p>  <img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731112215.png" alt></p><p><strong>配置内网域名映射：</strong></p><blockquote><p>192.168.25.201          mini61</p><p> 192.168.25.202          mini62</p><p>192.168.25.203          mini63</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@mini61 hadoop]# vi /etc/hosts</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731112408.png" alt></p><h4 id="配置ssh免密登陆"><a href="#配置ssh免密登陆" class="headerlink" title="配置ssh免密登陆"></a><strong>配置ssh免密登陆</strong></h4><p>下面为简单版，如首次接触Linux,请自行搜索查找教程</p><p>配置ssh免登陆</p><pre><code>#生成ssh免登陆密钥#进入到我的home目录cd ~/.ssh</code></pre><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa （四个回车）</span><br><span class="line">执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</span><br><span class="line">将公钥拷贝到要免密登陆的目标机器上</span><br><span class="line">ssh-copy-id localhost</span><br><span class="line">---------------------------</span><br><span class="line">ssh免登陆：</span><br><span class="line">生成key:</span><br><span class="line">ssh-keygen</span><br><span class="line">复制从A复制到B上:</span><br><span class="line">ssh-copy-id B</span><br><span class="line">验证：</span><br><span class="line">ssh localhost/exit，ps -e|grep ssh</span><br><span class="line">ssh A  #在B中执行</span><br></pre></td></tr></table></figure><h4 id="配置-关闭防火墙"><a href="#配置-关闭防火墙" class="headerlink" title="配置/关闭防火墙"></a><strong>配置/关闭防火墙</strong></h4><p>下面为 <code>CentOS 6</code> 版本关闭防火墙的方法，如使用的是 <code>CentOS 7</code> 请自行百度</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>查看防火墙状态</span><br><span class="line">service iptables status</span><br><span class="line"><span class="meta">#</span>关闭防火墙</span><br><span class="line">service iptables stop</span><br><span class="line"><span class="meta">#</span>查看防火墙开机启动状态</span><br><span class="line">chkconfig iptables --list</span><br><span class="line"><span class="meta">#</span>关闭防火墙开机启动</span><br><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure><h4 id="Jdk环境安装"><a href="#Jdk环境安装" class="headerlink" title="Jdk环境安装"></a>Jdk环境安装</h4><p><strong>如未安装过</strong>(说明还没有学习过<code>Java</code>,那么建议先学习 <code>Java</code> 基础)，请参考百度：<a href="https://jingyan.baidu.com/article/2a138328fda0b2074a134f20.html" target="_blank" rel="noopener">JDK安装</a></p><ul><li><p>上传jdk安装包</p></li><li><p>规划安装目录  <code>/home/hadoop/app/jdk_1.7.45</code></p></li><li><p>解压安装包</p></li><li><p>配置环境变量 /etc/profile</p></li></ul><h4 id="HADOOP安装部署"><a href="#HADOOP安装部署" class="headerlink" title="HADOOP安装部署"></a>HADOOP安装部署</h4><p>配置好一台后使用 <code>scp</code> 命令发送到另外两台即可</p><blockquote><p>下载链接前面已提及</p></blockquote><ul><li>上传HADOOP安装包</li><li>规划安装目录  <code>/home/hadoop/app/hadoop-2.6.4</code></li><li>解压安装包</li></ul><p>安装方法类似 <code>JDK</code> 本次我下载的是hadoop <code>2.6.4</code> 版本</p><p>本次配置 <code>/etc/profile</code> 文件参考如下(请根据需要修改)：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/hadoop/app/jdk1.7.0_45</span><br><span class="line">export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.4</span><br><span class="line"></span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</span><br></pre></td></tr></table></figure><h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><p>进入 <strong>$HADOOP_HOME/etc/hadoop/</strong> 目录下：</p><p><code>vi  hadoop-env.sh</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> The java implementation to use.</span><br><span class="line">export JAVA_HOME=/home/hadoop/apps/jdk1.7.0_45</span><br></pre></td></tr></table></figure><p><code>vi  core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mini61:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadpdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vi  hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadpdata/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadpdata/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.secondary.http.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mini61:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>mapred-site.xml 需要先修改名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure><p><code>vi  mapred-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vi  yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mini61<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>vi  salves</code>  （作为DataNode的主机名）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mini62</span><br><span class="line">mini63</span><br></pre></td></tr></table></figure><h4 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h4><p>切回 <strong>hadoop</strong> 用户</p><p><code>初始化HDFS</code></p><p>在 <code>mini61</code> 上执行格式化 这里 <code>mini61</code> 我作为 <code>master</code>， 另外两台作为从节点 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop  namenode  -format</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731120058.png" alt></p><p><strong>如图所示就启动<code>格式化</code>成功了。</strong></p><p><strong><code>启动HDFS</code></strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure><p><code>启动YARN</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><p>如果直接执行 <code>start-all.sh</code> 就代表 同时启动 <code>HDFS</code> 和 <code>Yarn</code></p><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731120245.png" alt></p><p>三个节点分别执行 jps  查看进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini61 ~]$ jps</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731123830.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini62 ~]$ jps</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731121516.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@mini63 ~]$ jps</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/zranfly/tuchuang/master/img/20190731123932.png" alt></p><p>详细教程可参看：<a href="https://blog.csdn.net/iflytop/article/details/82413694" target="_blank" rel="noopener">Hadoop集群搭建</a>   （适合小白）</p><p><strong>注意：本人也是初学者，作为学习笔记，如有错漏，敬请见谅。</strong></p>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1.认识Hadoop</title>
      <link href="/hadoop/f868e049.html"/>
      <url>/hadoop/f868e049.html</url>
      
        <content type="html"><![CDATA[<center><h3>大数据专栏 | 认识Hadoop</h3></center><hr><h3 id="Hadoop简介"><a href="#Hadoop简介" class="headerlink" title="Hadoop简介"></a>Hadoop简介</h3><ul><li><p>HADOOP是apache旗下的一套开源<strong>软件平台</strong></p></li><li><p>HADOOP提供的功能：利用服务器集群，根据用户的自定义业务逻辑，<strong>对海量数据进行分布式处理</strong></p></li><li><p>HADOOP的核心组件有</p><ul><li><p>HDFS（分布式文件系统）</p></li><li><p>YARN（运算资源调度系统）</p></li><li><p>MAPREDUCE（分布式运算编程框架）</p></li></ul></li><li><p>广义上来说，HADOOP通常是指一个更广泛的概念——HADOOP生态圈</p></li></ul><h3 id="Hadoop产生背景"><a href="#Hadoop产生背景" class="headerlink" title="Hadoop产生背景"></a>Hadoop产生背景</h3><ul><li><p>Hadoop<strong>最早起源于Nutch</strong>。Nutch的设计目标是构建一个大型的全网搜索引擎，包括网页抓取、索引、查询等功能，但随着抓取网页数量的增加，<strong>遇到了严重的可扩展性问题——</strong>如何解决数十亿网页的存储和索引问题。</p></li><li><p>2003年、2004年<strong>谷歌发表的两篇论文为该问题提供了可行的解决方案</strong>。</p><ul><li><p>分布式文件系统（GFS），可用于处理海量网页的<strong>存储</strong></p></li><li><p>分布式计算框架MAPREDUCE，可用于处理海量网页的<strong>索引计算</strong>问题。</p></li></ul></li><li><p>Nutch的开发人员完成了相应的<strong>开源实现HDFS和MAPREDUCE</strong>，并从Nutch中剥离成为独立项目Hadoop，到2008年1月，Hadoop成为Apache顶级项目，迎来了它的快速发展期。</p></li></ul><h3 id="Hadoop在大数据、云计算中的位置和关系"><a href="#Hadoop在大数据、云计算中的位置和关系" class="headerlink" title="Hadoop在大数据、云计算中的位置和关系"></a>Hadoop在大数据、云计算中的位置和关系</h3><ul><li><p>云计算是分布式计算、并行计算、网格计算、多核计算、网络存储、虚拟化、负载均衡等传统计算机技术和互联网技术融合发展的产物。借助IaaS(基础设施即服务)、PaaS(平台即服务)、SaaS（软件即服务）等业务模式，把强大的计算能力提供给终端用户。</p></li><li><p>现阶段，云计算的<strong>两大底层支撑技术</strong>为“<strong>虚拟化</strong>”和“<strong>大数据技术</strong>”</p></li><li><p>而Hadoop则是云计算的PaaS层的解决方案之一，并不等同于PaaS，更不等同于云计算本身。</p></li></ul><h3 id="Hadoop应用案例"><a href="#Hadoop应用案例" class="headerlink" title="Hadoop应用案例"></a>Hadoop应用案例</h3><ul><li><strong>应用于数据服务基础平台建设</strong></li><li><strong>用于用户画像</strong></li><li><strong>用于网站点击流日志数据挖掘</strong></li></ul><h3 id="Hadoop生态圈重点组件"><a href="#Hadoop生态圈重点组件" class="headerlink" title="Hadoop生态圈重点组件"></a>Hadoop生态圈重点组件</h3><p>重点组件：</p><ul><li><p>HDFS：分布式文件系统</p></li><li><p>MAPREDUCE：分布式运算程序开发框架</p></li><li><p>HIVE：基于大数据技术（文件系统+运算框架）的SQL数据仓库工具</p></li><li><p>HBASE：基于HADOOP的分布式海量数据库</p></li><li><p><strong>ZOOKEEPER：分布式协调服务基础组件</strong></p></li><li><p>Mahout：基于mapreduce/spark/flink等分布式运算框架的机器学习算法库</p></li><li><p>Oozie：工作流调度框架</p></li><li><p>Sqoop：数据导入导出工具</p></li><li><p>Flume：日志数据采集框架</p></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
